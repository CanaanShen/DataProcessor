
      Bayesian networks are popular for modeling conditional distributions of variables and causal relationships, especially in various biological settings such as protein interactions, gene regulatory networks and microbial interactions. Previous Bayesian network structure learning algorithms treat those variables with similar tendency separately. In this paper, we propose a grouped sparse Gaussian Bayesian network GSGBN) structure learning algorithm that creates the optimal Gaussian Bayesian network based on three assumptions: (i) variables follow a multivariate Gaussian distribution, (ii) the network only contains a few edges (sparse), (iii) similar variables have less-divergent sets of parents, while not-so-similar ones should have divergent sets of parents (variable grouping). We use $L_1$ regularization to make the learned network sparse, and another term to incorporate shared information among variables (a series of Laplacian distributions for penalizing the differences of variables' parent sets). For similar variables, GSGBN tends to penalize the differences of similar variables' parent sets more, compared to those not-so-similar variables' parent sets. The similarity of variables is learned from the data by alternating optimization, without prior domain knowledge. Based on this new definition of the optimal Bayesian network, a coordinate descent algorithm and a projected gradient descent algorithm are developed to obtain information about edges of the network and also similarity of variables. Experimental results on both simulated and real datasets show that GSGBN can have substantially superior prediction performance in terms of sensitivity and specificity of the learned network when compared to several existing algorithms.
      