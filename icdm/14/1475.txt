
      Given a high-dimensional and large-scale tensor, how can we decompose it into latent factors? Can we process it on commodity computers with limited memory? These questions are closely related to recommendation systems, where rating data have been modeled not as a matrix but as a tensor as extra information such as time and location has become available. This increase in the dimension of web-scale recommendation problems requires tensor factorization methods scalable with both the dimension and the size of a tensor. In this paper, we propose two distributed tensor factorization methods, CDTF and SALS. Both methods are scalable with all aspects of data, and they show an interesting trade-off between convergence speed and memory requirements. CDTF is based on coordinate descent and updates one parameter at a time; and SALS generalizes on the number of parameters updated at a time. On our experiment, only our methods factorize a 5-dimension tensor with 1B observable entries, 10M mode length, and 1K rank, while all other state-of-the art methods fail. Moreover, our methods require several orders of magnitude less memory than the competitors. We implement our methods on MAPREDUCE with two widely applicable optimization techniques: local disk caching and greedy row assignment. They speed up our methods up to 98.2X and also the competitors up to 5.9X.
      