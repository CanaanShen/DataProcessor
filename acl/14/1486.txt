This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers.
We perform unsupervised learning using noise-contrastive estimation [15, 26], which utilizes artificially generated negative samples.
Our alignment model is directional, similar to the generative IBM models [4].
To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training.
The RNN-based model outperforms the feed-forward neural network-based model [40] as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.